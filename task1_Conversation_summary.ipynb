{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install dependencies**"
      ],
      "metadata": {
        "id": "2haVlKZiLXkh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qSH_8yL6j3m",
        "outputId": "f07267db-c41b-47fd-f00f-b46d0e8f321f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.31.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
            "Downloading groq-0.31.1-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.31.1\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.106.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install groq\n",
        "!pip install requests\n",
        "!pip install openai requests"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "xi6kAu-hOThY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import requests"
      ],
      "metadata": {
        "id": "oVtTAMFGK1vZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Add Key**"
      ],
      "metadata": {
        "id": "fS7YbXWTOZKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace YOUR_API_KEY_HERE with your actual Groq API key\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "\n",
        "# Initialize client\n",
        "client = openai.OpenAI(\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=os.environ.get(\"GROQ_API_KEY\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "Kx5TJvbC7mqs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking Available Models**"
      ],
      "metadata": {
        "id": "mRduAi0kOeQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"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\"\n",
        "url = \"https://api.groq.com/openai/v1/models\"\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {api_key}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    data = response.json()\n",
        "    models_list = data.get(\"data\", [])\n",
        "    print(\"Available models:\")\n",
        "    for model in models_list:\n",
        "        print(f\"- {model['id']}\")\n",
        "else:\n",
        "    print(f\"Failed to fetch models: {response.status_code}\")\n",
        "    print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meJp6bquBXgq",
        "outputId": "f5263635-0dcc-45dc-f457-b54220d7168a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available models:\n",
            "- meta-llama/llama-guard-4-12b\n",
            "- whisper-large-v3-turbo\n",
            "- meta-llama/llama-prompt-guard-2-86m\n",
            "- llama-3.1-8b-instant\n",
            "- groq/compound-mini\n",
            "- playai-tts-arabic\n",
            "- meta-llama/llama-4-maverick-17b-128e-instruct\n",
            "- moonshotai/kimi-k2-instruct\n",
            "- openai/gpt-oss-120b\n",
            "- openai/gpt-oss-20b\n",
            "- whisper-large-v3\n",
            "- groq/compound\n",
            "- llama-3.3-70b-versatile\n",
            "- meta-llama/llama-prompt-guard-2-22m\n",
            "- qwen/qwen3-32b\n",
            "- playai-tts\n",
            "- allam-2-7b\n",
            "- meta-llama/llama-4-scout-17b-16e-instruct\n",
            "- moonshotai/kimi-k2-instruct-0905\n",
            "- gemma2-9b-it\n",
            "- deepseek-r1-distill-llama-70b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize conversation history**"
      ],
      "metadata": {
        "id": "Ya9OzHfiMLD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conversation history storage\n",
        "conversation_history = []\n",
        "\n",
        "# Add messages\n",
        "def add_message(role, content):\n",
        "    \"\"\"\n",
        "    Add a message to the conversation history.\n",
        "\n",
        "    Args:\n",
        "        role (str): 'user' or 'assistant'\n",
        "        content (str): Text content of the message\n",
        "    \"\"\"\n",
        "    conversation_history.append({\"role\": role, \"content\": content})\n"
      ],
      "metadata": {
        "id": "2He8BWkH7mvy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Truncate conversation history**"
      ],
      "metadata": {
        "id": "xMcqcLtWui_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def truncate_history(history, max_turns=None, max_chars=None, max_words=None):\n",
        "    \"\"\"\n",
        "    Truncate conversation history based on turns, characters, or words.\n",
        "    Supports partial truncation for long messages.\n",
        "\n",
        "    Args:\n",
        "        history (list): List of message dictionaries\n",
        "        max_turns (int, optional): Maximum number of conversation turns\n",
        "        max_chars (int, optional): Maximum total characters\n",
        "        max_words (int, optional): Maximum total words\n",
        "\n",
        "    Returns:\n",
        "        list: Truncated conversation history\n",
        "    \"\"\"\n",
        "    # Truncate by number of turns\n",
        "    if max_turns:\n",
        "        history = history[-max_turns:]\n",
        "\n",
        "    # Truncate by number of characters (partial messages allowed)\n",
        "    if max_chars:\n",
        "        total_chars = 0\n",
        "        truncated = []\n",
        "        for msg in reversed(history):\n",
        "            msg_len = len(msg['content'])\n",
        "            remaining = max_chars - total_chars\n",
        "            if remaining <= 0:\n",
        "                break\n",
        "            if msg_len > remaining:\n",
        "                # Keep only the last 'remaining' characters of this message\n",
        "                msg_copy = msg.copy()\n",
        "                msg_copy['content'] = msg_copy['content'][-remaining:]\n",
        "                truncated.insert(0, msg_copy)\n",
        "                break\n",
        "            truncated.insert(0, msg)\n",
        "            total_chars += msg_len\n",
        "        history = truncated\n",
        "\n",
        "    # Truncate by number of words (partial messages allowed)\n",
        "    if max_words:\n",
        "        total_words = 0\n",
        "        truncated = []\n",
        "        for msg in reversed(history):\n",
        "            words = msg['content'].split()\n",
        "            remaining = max_words - total_words\n",
        "            if remaining <= 0:\n",
        "                break\n",
        "            if len(words) > remaining:\n",
        "                # Keep only the last 'remaining' words of this message\n",
        "                msg_copy = msg.copy()\n",
        "                msg_copy['content'] = ' '.join(words[-remaining:])\n",
        "                truncated.insert(0, msg_copy)\n",
        "                break\n",
        "            truncated.insert(0, msg)\n",
        "            total_words += len(words)\n",
        "        history = truncated\n",
        "\n",
        "    return history\n"
      ],
      "metadata": {
        "id": "d14PgyKXMc_S"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summarize conversation using Groq API**"
      ],
      "metadata": {
        "id": "ns5SMBGCM8TQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_history(history):\n",
        "    messages_text = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in history])\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"groq/compound-mini\",\n",
        "        messages=[{\"role\":\"user\", \"content\": f\"Summarize this conversation:\\n{messages_text}\"}],\n",
        "        temperature=0.3,          # Low randomness for concise summary\n",
        "        max_completion_tokens=500, # Enough for summary\n",
        "        top_p=1,\n",
        "        stream=False\n",
        "    )\n",
        "    summary = response.choices[0].message.content\n",
        "    return summary"
      ],
      "metadata": {
        "id": "RV-RG2fIMtph"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Periodic summarization after every k messagest**"
      ],
      "metadata": {
        "id": "IYGPRbRJNAQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def maybe_summarize(history, k=3):\n",
        "    \"\"\"\n",
        "    Append a summary after every k messages instead of replacing the history.\n",
        "    \"\"\"\n",
        "    if len(history) % k == 0:\n",
        "        summary = summarize_history(history)\n",
        "        print(f\"\\n[Conversation summarized after {k} messages]:\\n{summary}\\n\")\n",
        "        # Append the summary as a system message\n",
        "        history.append({\"role\": \"system\", \"content\": f\"Summary so far: {summary}\"})\n",
        "    return history\n"
      ],
      "metadata": {
        "id": "Ygc0qXRnMvlw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Demonstration with multiple conversation samples**"
      ],
      "metadata": {
        "id": "3FFK2DqiNPXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_conversations = [\n",
        "    [(\"user\", \"Hi, I want to learn about AI.\"),\n",
        "     (\"assistant\", \"Sure! What aspect of AI?\"),\n",
        "     (\"user\", \"Neural networks.\"),\n",
        "     (\"assistant\", \"Neural networks are models inspired by the human brain.\"),\n",
        "     (\"user\", \"Thanks!\")],\n",
        "\n",
        "    [(\"user\", \"Hello, I need help with Python.\"),\n",
        "     (\"assistant\", \"Python is great! What do you want to know?\"),\n",
        "     (\"user\", \"How to read files.\"),\n",
        "     (\"assistant\", \"You can use open() function to read files.\"),\n",
        "     (\"user\", \"Got it!\")],\n",
        "\n",
        "    [(\"user\", \"Hey, tell me about machine learning.\"),\n",
        "     (\"assistant\", \"ML is about training models to learn patterns.\"),\n",
        "     (\"user\", \"Give me an example.\"),\n",
        "     (\"assistant\", \"Image recognition is a common ML example.\"),\n",
        "     (\"user\", \"Thanks!\")]\n",
        "]\n",
        "\n",
        "# Run demo\n",
        "for idx, conv in enumerate(sample_conversations):\n",
        "    print(f\"\\n--- Conversation Sample {idx+1} ---\")\n",
        "\n",
        "    # Reset conversation history for each sample\n",
        "    conversation_history = []\n",
        "\n",
        "    for role, text in conv:\n",
        "        add_message(role, text)\n",
        "\n",
        "    # Summarize at the end of this conversation sample (instead of after each message)\n",
        "    conversation_history = maybe_summarize(conversation_history, k=len(conv))\n",
        "\n",
        "    # Show different truncation settings\n",
        "    print(\"\\nTruncated to last 4 messages:\")\n",
        "    truncated_turns = truncate_history(conversation_history, max_turns=4)\n",
        "    for msg in truncated_turns:\n",
        "        print(f\"{msg['role']}: {msg['content']}\")\n",
        "\n",
        "    print(\"\\nTruncated to last 50 characters:\")\n",
        "    truncated_chars = truncate_history(conversation_history, max_chars=50)\n",
        "    for msg in truncated_chars:\n",
        "        print(f\"{msg['role']}: {msg['content']}\")\n",
        "\n",
        "    print(\"\\nTruncated to last 20 words:\")\n",
        "    truncated_words = truncate_history(conversation_history, max_words=20)\n",
        "    for msg in truncated_words:\n",
        "        print(f\"{msg['role']}: {msg['content']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-HSxdyT7myi",
        "outputId": "9c88a728-ad5b-41a1-b68c-70de45b99626"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Conversation Sample 1 ---\n",
            "\n",
            "[Conversation summarized after 5 messages]:\n",
            "We've just started. You've expressed interest in learning about AI, specifically neural networks. I've provided a brief introduction. We can continue exploring the topic if you'd like.\n",
            "\n",
            "\n",
            "Truncated to last 4 messages:\n",
            "user: Neural networks.\n",
            "assistant: Neural networks are models inspired by the human brain.\n",
            "user: Thanks!\n",
            "system: Summary so far: We've just started. You've expressed interest in learning about AI, specifically neural networks. I've provided a brief introduction. We can continue exploring the topic if you'd like.\n",
            "\n",
            "Truncated to last 50 characters:\n",
            "system: We can continue exploring the topic if you'd like.\n",
            "\n",
            "Truncated to last 20 words:\n",
            "system: learning about AI, specifically neural networks. I've provided a brief introduction. We can continue exploring the topic if you'd like.\n",
            "\n",
            "--- Conversation Sample 2 ---\n",
            "\n",
            "[Conversation summarized after 5 messages]:\n",
            "The conversation is over. The user asked for help with Python, specifically how to read files, and I provided a brief answer using the `open()` function. The user confirmed they understood. There is no further conversation to summarize. How can I help you with Python?\n",
            "\n",
            "\n",
            "Truncated to last 4 messages:\n",
            "user: How to read files.\n",
            "assistant: You can use open() function to read files.\n",
            "user: Got it!\n",
            "system: Summary so far: The conversation is over. The user asked for help with Python, specifically how to read files, and I provided a brief answer using the `open()` function. The user confirmed they understood. There is no further conversation to summarize. How can I help you with Python?\n",
            "\n",
            "Truncated to last 50 characters:\n",
            "system: tion to summarize. How can I help you with Python?\n",
            "\n",
            "Truncated to last 20 words:\n",
            "system: function. The user confirmed they understood. There is no further conversation to summarize. How can I help you with Python?\n",
            "\n",
            "--- Conversation Sample 3 ---\n",
            "\n",
            "[Conversation summarized after 5 messages]:\n",
            "The conversation is about machine learning (ML). The user asked for a general overview, and I provided a brief summary. They then asked for an example, and I gave a simple one: image recognition. The conversation was brief and didn't require in-depth discussion.\n",
            "\n",
            "\n",
            "Truncated to last 4 messages:\n",
            "user: Give me an example.\n",
            "assistant: Image recognition is a common ML example.\n",
            "user: Thanks!\n",
            "system: Summary so far: The conversation is about machine learning (ML). The user asked for a general overview, and I provided a brief summary. They then asked for an example, and I gave a simple one: image recognition. The conversation was brief and didn't require in-depth discussion.\n",
            "\n",
            "Truncated to last 50 characters:\n",
            "system:  was brief and didn't require in-depth discussion.\n",
            "\n",
            "Truncated to last 20 words:\n",
            "system: for an example, and I gave a simple one: image recognition. The conversation was brief and didn't require in-depth discussion.\n"
          ]
        }
      ]
    }
  ]
}